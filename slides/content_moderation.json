{
	"slide_item": [
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.001.jpeg",
			"text": "Today, I'll be talking to you all about building technology for society and talk through some of the complications and consequences that one needs to consider when doing so, looking specifically at the case of automated content moderation. I am an assistant professor in UW CSE and lead a research group on building social and collaborative systems."
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.002.jpeg",
			"text": "So to start out - what is content moderation? And why do we need it? Why is it an important topic today?"
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.003.jpeg",
			"text": "Content moderation isn't just removing bad stuff, though that's part of it, and it's the part that you've probably heard about the most. The first thing is to remember that content moderation has been with us as long as we've had content, or media of any kind. A long time ago, journalists and TV and radio producers had editors and regulators that looked over their work and made decisions about what would get published or broadcasted. At that time, there weren't very many avenues for media consumption so these so called \"gatekeepers\" had a lot of editorial control over what people saw, and as a result we didn't have a lot of diversity at the time in media. Of course this whole thing has turned on its head thanks to social media."
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.004.jpeg",
			"text": "And a lot of what social media has changed has been great. As I mentioned there has been more diversity in what's out there in terms of elevating minority voices. And for a long time there was this really positive, \"we're doing good for society\", \"we're bringing the world together\" kind of feeling about social media back in the early to mid 2000's. See these two quotes from around that time by Mark Zuckerberg and Tim Berners Lee. It's kind of eye-opening to read these rosy quotes today, knowing the general sentiment around social media now!"
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.005.jpeg",
			"text": "And of course it's not the case that everything was great, and then everything went terrible. Even back then, there were lots of people, in particular marginalized groups like women in gaming, that were experiencing horrible issues of harassment, stalking, doxing, nonconsensual sexual imagery in online spaces and yelling about it to all of these platforms and were basically ignored for a long time. But it's really starting to become more recognized in the mainstream more recently, and it's a problem that platforms are taking more seriously in the last several years after a number of incidents that have happened and a lot of public scrutiny."
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.006.jpeg",
			"text": "One really clear recent (and ongoing) example where this has come up is election misinformation. Starting earlier this year in 2020, many major platforms have added policies to label, hide, and reduce sharing of election misinformation. Actually several of these policies were already on the books for a while but there was a carveout that exempted politicians and content that was deemed of significant public interest (to keep up). Finally earlier this year, some of that moderation was actually imposed on some major politicians (as you see here in the screenshot) as well."
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.007.jpeg",
			"text": "Another kind of misinformation that has been really concerning is conspiracy theories, some of which are really dangerous to our public health, like conspiracies that say that vaccines cause autism or that COVID is a hoax. As you can see in the newspaper articles I have here, it's not just Facebook and Twitter, you've got Youtube, Patreon, Vimeo, Spotify, Apple Podcasts&hellip;again ANYWHERE you host content, you have to be thinking about this today."
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.008.jpeg",
			"text": "And there's also been a lot of issues with hate speech against minorities, harassment of people particularly marginalized groups, incitement to violence, like in the case of the Rohingya genocide against Muslims in Myanmar, where anti-Muslim content was allowed to spread in that country via social media. There's also threats, stalking, doxing (releasing of one's private info publicly), just general incivility, and also bullying. Some groups have taken all of this and put it in one big bucket labeled \"Toxicity\", and we'll get into that decision further."
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.009.jpeg",
			"text": "So that's just a taste of the problems that are related to content moderation today. We could talk about this all day. But how would you address some of these problems. Say you're Mark Zuckerberg, or say you're a developer at one of these platforms. You helped build this tool, and now it's causing societal issues like harassment against minorities, maybe even contributing to genocide. What do you do to address the problem? Here's Mark at a congressional hearing last year on the topic of AI. Link to the embedded video: <a href=\"https://www.youtube.com/watch?v=8sys7In_qDs\">https://www.youtube.com/watch?v=8sys7In_qDs</a> He is suggesting that in the future AI could do content moderation to remove hate speech."
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.010.jpeg",
			"text": "And actually you have some experience with this, because in CSE 143, you had an assignment to build a toxicity classifier. The assignment involved building a really simple model to detect toxicity using decision trees so it's a bit more of a toy example, but even so, you probably noticed some things. What was hard about your task? Did you have any concerns with the task or the way it was posed to you? Would you ever deploy something like what you built? Link to assignment: <a href=\"https://courses.cs.washington.edu/courses/cse143/20au/text-classifier/\">https://courses.cs.washington.edu/courses/cse143/20au/text-classifier/</a>"
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.011.jpeg",
			"text": "One thing to keep in mind is that content moderation is really tricky even for humans! Here are some of Facebook's rules regarding nudity on the platform and how they've shifted around over the years. As you can see something like \"no nudity\" actually has tons of edge cases that you and I probably wouldn't have thought about and Facebook definitely didn't think about until some people somewhere got really angry about it. <a href=\"https://www.facebook.com/communitystandards/adult_nudity_sexual_activity\">Link to Facebook nudity standards</a>."
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.012.jpeg",
			"text": "One question you need to consider is - where is your training data coming from? You might get high accuracy while training and that accuracy dives when you deploy because of lots of unseen content. In the case of the Perspective dataset on Kaggle that you used for your assignment, the data comes from discussions that happened on Wikipedia. Wikipedia is known to have some issues with diversity, as the community is skewed towards white, male, and Western, and will have very different speech patterns, and very specific types of discussion compared to other places. How would training a model based on that kind of data affect your model? Kaggle challenge: <a href=\"https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge\">https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge</a>"
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.013.jpeg",
			"text": "Another important aspect to consider is - who is doing the labeling? Bias in your model can arise from who your annotators are and what they know, either what they know from their life experience, which can be problematic if they're not very diverse, or what you told them or trained them to do. Like in this case shown on the slide, these researchers at UW found that annotators labeled data with African American English (AAE) as hate speech more often than a non-AAE equivalent, but this bias was reduced when they were actually told that a particular piece of text was AAE. <a href=\"https://homes.cs.washington.edu/~msap/pdfs/sap2019risk.pdf\">Link to paper</a>. Sometimes you get or use data online and it actually doesn't tell you this information. Like I did some searching online, and I'm not actually sure who did the labeling for the Wikipedia dataset that Perspective is based on. It could be something like Amazon Mechanical Turk (which by the way also has a skewed demographic) or Google internal contractors."
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.014.jpeg",
			"text": "Another issue is - what are your labels and label definitions? As I mentioned earlier, the term \"toxicity\" is kind of this all-encompassing word that could potentially mean a bunch of things. This is how Perspective defines it (on the left). But if you look at their label categories they have both \"toxic\" and \"severe_toxic\" - so what's the difference between those two? All of these questions about classifications are really important, not just for machine learning. Classifications affect everything - from your medical records, your tax and census documents, how we define and bin gender, race, education on surveys and social media profiles, and more. This book \"Sorting Things Out\" by Bowker and Star is a classic book about this topic."
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.015.jpeg",
			"text": "Another concern is that sometimes people have legitimate disagreements about a certain label or definition. As one example, we interviewed a number of people who deal with online harassment a few years ago for one of the papers I had you all read (on <a href=\"https://homes.cs.washington.edu/~axz/papers/squadbox.pdf\">Squadbox</a>). And in those interviews, we interviewed a wide variety of people, from Youtube celebrities to journalists to scientists to politicians activists, to even just a partner dealing with an ex. It turns out these people all have very different ideas about what is harassment. And a lot of the time it wasn't always about the content of the message but the context - how frequently this person had messaged them, the nature of their relationship, whether they had told them to stop."
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.016.jpeg",
			"text": "So are there some things you can do to mitigate some of these issues? I'll just outline a few guidelines. But in general, in your assignment you mostly focused on the modeling and grabbed a dataset from off the web. But that's not how it is in a real world context (or at least it shouldn't be!) Actually a lot more time and effort is spent on the data collection, data labeling, and data cleaning side of the process. From surveys of data scientists, they've found that they spend about 80% of their time in data cleaning and organizing and 20% doing modeling, and that's not even counting the data gathering process. Basically know your data - understand the community, understand the context, and make sure it actually fits your use case."
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.017.jpeg",
			"text": "There have been some efforts to get people who do release datasets on the web to actually provide some of this information about who labeled it, how was it labeled, where the data is from, because of how important those questions are for determining dataset bias. Here are links regarding these efforts - Datasheets for Datasets: <a href=\"https://arxiv.org/pdf/1803.09010.pdf\">https://arxiv.org/pdf/1803.09010.pdf</a> and Data Nutrition Project: <a href=\"https://datanutrition.org/\">https://datanutrition.org/</a>"
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.018.jpeg",
			"text": "And finally, when thinking about picking your labels and coming up with label definitions, you should not rely on your own intuition or that of your team for determining this. Get feedback from a diverse set of your future users, learn from experts, and test out your labels with your annotators."
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.019.jpeg",
			"text": "So far, we've made the assumption that we are using machine learning classifiers trained on large amounts of labeled data to classify \"toxic\" content. But zooming out, what have we signed up for when we make that assumption?"
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.020.jpeg",
			"text": "I love this art piece by Kate Crawford and Vladan Joler because it really shows how in a production AI system, there are in actuality so many moving pieces that come together to make it work. And it brings into focus just how many natural resources and human resources we use to build and deploy a model and how much that's kind of hidden away when you just see the model working. No model is free even though it might feel that way sometimes. Link: <a href=\"https://anatomyof.ai/\">https://anatomyof.ai/</a>"
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.021.jpeg",
			"text": "Focusing in on a tiny piece of that diagram, let's look just at the human resources you take on. In order to to train a model, you need lots and lots of data! To get data, you need annotators. So now you need to employ lots and lots of annotators, to both label the data as well as continuously check the outputs of the model."
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.022.jpeg",
			"text": "How many people are \"lots\"? In the case of Facebook, that's around 15,000 people. Unfortunately, the work conditions of these content moderators leaves a lot to be desired. What are some problems with the way annotators are recruited and employed today?"
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.023.jpeg",
			"text": "Link to video: <a href=\"https://www.youtube.com/watch?v=bDnjiNCtFk4\">https://www.youtube.com/watch?v=bDnjiNCtFk4</a> Suggested snippets to watch (these snippets avoid mention of specific traumatic content): 1:12 - 2:54, 6:33-10:12, 12:47- end."
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.024.jpeg",
			"text": "There has been a lot of attention paid to contract workers and commercial content moderators that power AI systems and conduct moderation in the last several years. These are some great books, and the last one is a documentary that came out recently. The first book \"Ghost Work\" is by researchers at Microsoft Research. Their book goes in depth to understand the people who sit behind sites like Amazon Mechanical Turk to do small tasks and data labeling. \"Behind the Screen\" is about commercial content moderators and builds on a decade of interviews Sarah Roberts conducted with these paid moderators. And \"The Cleaners\" is again about the same topic but in documentary form."
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.025.jpeg",
			"text": "Another point I wanted to make is that it isn't just the case that you go and label a ton of data and train your model and then you're done. You can just reap the rewards while the model churns away. Actually, in most production systems, you need to be continually gathering and labeling new data because the world changes - new events happen adding new vocabulary, language shifts over time, and your model gets out-of-date. Also in a production setting, you need to keep checking to make sure your model is of high quality, so every now and then you still have to do a human evaluation check. This is kind of a mind-blowing revelation - Google has said that actually every single day 15% of their searches have never been searched before!"
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.026.jpeg",
			"text": "What to do about all this? First - get rid of the notion that you will be able to remove the need for humans to do this work. Especially in these high social situations, you will ALWAYS need people involved! Since people are always going to be around, it's important that we think about how to better improve their working conditions. How can policies support them? How can technologies be used to help them do their work instead of trying to just automate them away?"
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.027.jpeg",
			"text": "Let's zoom out again! Say you've decided to use an ML classifier for content moderation. You've figured out how to deploy the model so that it's continually updated and evaluated. Now what? How do you actually use the model that you built on your platform? What actually happens to the content that is deemed toxic or the person who has broken the rules? The basic assumption here is that you run your model over the platform content as it's coming in continually, and for all the content that it says is toxic, you take it down."
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.028.jpeg",
			"text": "But actually, that's just one way to do it. There are tons of other things you could do and each of these have different ramifications for the violator, the people who are harmed, and for the content and how it spreads or doesn't spread. Here are Twitter's range of enforcement options for example. There's escalation in punishment for repeated bad behavior."
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.029.jpeg",
			"text": "Here's another diagram that was made by Alex Stamos, Facebook's former Chief Security Officer. It's a suggestion by him and not a diagram of how things are actually done in a particular platform today. But here, he's arguing for a greater severity of punishment if your violating content is posted on a more widespread place."
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.030.jpeg",
			"text": "Another question is what do you do with the person who broke the rules? Should you just ban them right away? Take down their content and give them a warning? Implicit in these questions is - who is this person and why did they do what they did? Are they a \"bad actor\" with ill intent or did they make a mistake because they didn't know the rules? In this work, researchers (including Shagun Jhaver, a current postdoc in our lab!) found that sometimes people who had their content taken down didn't even know that it had happened! And in fact, when moderators provided an explanation for why their content was removed, it reduced the odds of that person getting their post removed in the future. Link to paper: <a href=\"https://dl.acm.org/doi/pdf/10.1145/3359252\">https://dl.acm.org/doi/pdf/10.1145/3359252</a>"
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.031.jpeg",
			"text": "Here's another way you could use the model that's not after content is posted but BEFORE. There are some clear pros and cons to this kind of approach. One pro is that people who maybe didn't mean to be \"toxic\" or need a moment to reconsider could change their comment. But a con is that real-time feedback could allow bad actors to quickly test out different phrasings to get around the model (and since human language is so flexible, they will likely succeed). Link to article: <a href=\"https://www.wired.com/story/comments-section-clean-up-let-ai-tell-users-words-trash/\">https://www.wired.com/story/comments-section-clean-up-let-ai-tell-users-words-trash/</a>"
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.032.jpeg",
			"text": "Finally, there's been some research looking into alternative theories of justice, and whether they could apply here. Everything we've talked about so far assumes a punitive approach to content moderation. You get punished for breaking the rules. But what about a restorative justice approach, where we instead consider the harm done to individuals and the community, and have offenders work to repair that harm? In this case, the researchers found some support for people who have been harassed who want these kinds of approaches, such as requiring an apology or payment from the offender. <a href=\"https://deepblue.lib.umich.edu/bitstream/handle/2027.42/154685/SchoenebeckDrawingFromJustice.pdf?sequence=1\">Link to paper</a>."
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.033.jpeg",
			"text": "Ok, so thinking back to those considerations, what should you do? Consider gradations of enforcement that are tied to severity and repetition. Actually listen to people who have been harmed to understand their preferences. Allow for learning and improvement, and make clear the rules in a community. Sometimes this is all more work than just banning people right away, but remember that when you ban people that are acting terribly on your site, they can just go off and be terrible on another site, which is kicking the can down the road to some degree."
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.034.jpeg",
			"text": "Alright, zooming out one last time! You have a classifier for toxicity, you have a process for keeping it going, and you've now deployed it on your platform. What assumptions are you still making at this point?"
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.035.jpeg",
			"text": "One assumption you're making is that it is okay to have a model to cover all of the people on your site (which could be millions if you are large enough). On the plus side, you can claim consistency, since everyone is subject to the same model. But as a downside, you lose local differences in interpretation, and that \"consistency\" may mask inequities in user experience due to biases of the model. Take as an example the U.S. system of government. A consistent federally-deployed response to something like COVID-related restrictions might have been better than each state making decisions for itself. But in other cases (and some argue, also in this case), it's probably a good thing that we have mayors and governors that can craft rules suited to each jurisdiction. What's right for one area may not be right for another."
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.036.jpeg",
			"text": "Reddit is one example where there's the top-level platform layer of content moderation, but then there's this strong additional sublayer of content moderation where each subreddit can make their own rules, sometimes using machine learning, sometimes using hand crafted rules like word filters, and sometimes just via manually removing content."
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.037.jpeg",
			"text": "I'll also mention our Squadbox project (<a href=\"https://squadbox.org/\">https://squadbox.org/</a>). Remember how I mentioned earlier that we interviewed all these people who faced harassment? And how they had really different ideas about what is harassment? This told us that it would be a mistake to try and make a single model to describe harassment for everyone. Instead we wanted to give people the power to customize what they consider to be harassment. But also it's a lot of work for the person who's getting harassed, not to mention emotionally painful to read through these messages. So we came up with a friend-sourced moderation approach, where friends can help manually filter and generate blocklists for a person getting harassed. And actually we got this idea because people are already doing it (see quote)! and in the future we're interested in models where people can band together in a \"you help me, I help you\" way. Right now, we have a project in our lab looking at this idea for Youtube content creators and allowing them to collaborate on and share word filters and data to create models together."
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.038.jpeg",
			"text": "Another assumption that we're making is that we (meaning the developers, project managers, and policy teams behind the model) are the right people to make decisions about all those people on the platform. Is this process legitimate?"
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.039.jpeg",
			"text": "But then a question is - who should make the rules? Probably not just one person or a small team or even a single country's government. There are problems with all these approaches."
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.040.jpeg",
			"text": "Here's one idea that Facebook is trying out - a body of outside experts, specifically experts in human rights, law, and policy. This \"Supreme Court\" is starting out with 40 members. They are funded by a separate trust to maintain independence. Members have a 3 year term and are appointed by Facebook. Of course, there are issues with this approach too. The number of cases this group is going to be able to see are a drop in the bucket. 40 is really small, and entire continents are not represented. It remains to be seen how effective this board is going to be. Link: <a href=\"https://www.oversightboard.com/\">https://www.oversightboard.com/</a>"
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.041.jpeg",
			"text": "Another idea is to involve third party groups more that are independent and that can allow platforms to lean on their legitimacy somewhat. Some platforms have more recently partnered with third-party fact checkers for instance via Poynter's IFCN. Also there's been a lot of calls by civil society groups like Color of Change and the Anti-Defamation League as well as journalists trying to monitor election disinformation asking to have a bigger seat at the table. Academics too have been asking for access to data to be able to study platforms. The Santa Clara Principles is one example of a call made by civil society groups to major platforms to change their content moderation standards. Link: <a href=\"https://www.santaclaraprinciples.org/\">https://www.santaclaraprinciples.org/</a>"
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.042.jpeg",
			"text": "What about more democractic decision-making? Here's another project I participated in last year. If you think about it, platforms are actually pretty undemocratic when it comes to rulemaking and governance, despite all their rhetoric about being participatory. Imagine if we had things like juries or elections or local representatives on platforms. We did a project introducing the idea of digital juries for content moderation. We found that people find juries more legitimate than the status quo of top down algorithmic and paid moderation. I think there's still lots of avenues to explore interesting combinations of expert versus user versus paid versus algorithmic moderation and which is best suited for what. Link: <a href=\"http://digitaljuries.com/\">http://digitaljuries.com/</a>"
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.043.jpeg",
			"text": "I mentioned a few things but there's so much more going on in the world of content moderation! I don't think we're anywhere close to having a settled landscape or ossified way of doing things just yet."
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.044.jpeg",
			"text": "If you're a student at UW CSE, here are some other ways to learn more about these topics!"
		},
		{
			"image": "slides/content_moderation/guestlecture_contentmoderation.045.jpeg",
			"text": "And for more research and cool systems, come check out some of the work going on in my lab! Link: <a href=\"http://social.cs.washington.edu/\">http://social.cs.washington.edu/</a>"
		}
	]
}